{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camelyon Patch Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAVQPrpMvjYb"
   },
   "source": [
    "## Training a CNN\n",
    "We will first train a CNN on a dataset of [histopathology patches](https://en.wikipedia.org/wiki/Histopathology). This data corresponds to digitized microscopic analysis of tumor tissue, which has been divided into patches. The objective is to classify the patches into the ones containing tumor tissue, and ones not containing any tumor tissue. We will use the [PCAM dataset](https://github.com/basveeling/pcam) which consists of 96x96 pixel patches. We will only use the validation set (which contains 32768 patches and which should take about 0.8 GB of storage) in order to make the training faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NpZtqT5Fdoh"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import random\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchnet as tnt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets.utils import download_file_from_google_drive, _decompress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_5CIVY_B4tY"
   },
   "source": [
    "You can Download the dataset which is stored in a `.h5` file.\n",
    "The images can be download from [here](https://drive.google.com/uc?export=download&id=1hgshYGWK8V-eGRy8LToWJJgDU_rXWVJ3), and the labels from [here](https://drive.google.com/uc?export=download&id=1bH8ZRbhSVAhScTS0p9-ZzGnX91cHT3uO). Please then unzip the files and write the paths below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Uncomment the following cell and run it*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUz7fdZRjlhp"
   },
   "outputs": [],
   "source": [
    "# You can run the following cell to download the files on colab\n",
    "# base_folder = \"./\"\n",
    "# archive_name = \"camelyonpatch_level_2_split_valid_x.h5.gz\"\n",
    "# download_file_from_google_drive(\"1hgshYGWK8V-eGRy8LToWJJgDU_rXWVJ3\", base_folder, filename=archive_name, md5=\"d5b63470df7cfa627aeec8b9dc0c066e\")\n",
    "# _decompress(base_folder + archive_name)\n",
    "\n",
    "# archive_name = \"camelyonpatch_level_2_split_valid_y.h5.gz\"\n",
    "# download_file_from_google_drive(\"1bH8ZRbhSVAhScTS0p9-ZzGnX91cHT3uO\", base_folder, filename=archive_name, md5=\"2b85f58b927af9964a4c15b8f7e8f179\")\n",
    "# _decompress(base_folder + archive_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdOEHuVbnXFU"
   },
   "outputs": [],
   "source": [
    "IMAGES_PATH = \"data/camelyonpatch_level_2_split_valid_x.h5\"\n",
    "LABELS_PATH = \"data/camelyonpatch_level_2_split_valid_y.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8dIDByrHC8U"
   },
   "outputs": [],
   "source": [
    "images = np.array(h5py.File(IMAGES_PATH)['x'])\n",
    "labels = np.array([y.item() for y in h5py.File(LABELS_PATH)['y']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDcpDHPfIN1M"
   },
   "source": [
    "Now that we have the data, we will want to split it into a training and a validation set. For this, we will write a function which takes in as input the size of the dataset, and which will return the indices of the training set and the indices of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZejRMe3nIjA8"
   },
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taCCj9BAIMtC"
   },
   "outputs": [],
   "source": [
    "def get_split_indices(dataset_length, train_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Function which splits the data into tranining and validation sets.\n",
    "    arguments:\n",
    "        dataset_length [int]: number of elements in the dataset\n",
    "        train_ratio [float]: ratio of the dataset in the training set\n",
    "    returns:\n",
    "        train_indices [list]: list of indices in the training set (of size dataset_length*train_ratio)\n",
    "        val_indices [list]: list of indices in the validation set (of size dataset_length*(1-train_ratio))\n",
    "    \"\"\"\n",
    "    indices = list(range(dataset_length))\n",
    "    random.shuffle(indices)\n",
    "    return indices[:round(dataset_length*train_ratio)], indices[round(dataset_length*train_ratio):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0K4d2UmJg1h"
   },
   "outputs": [],
   "source": [
    "train_indices, val_indices = get_split_indices(len(labels))\n",
    "print(f\"There are {len(train_indices)} train indices and {len(val_indices)} validation indices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_T1sbPlFLQ9"
   },
   "source": [
    "Now let's write the dataset classes. We can add any type of data augmentation that you like. Please note that pytorch has an implemented PCAM dataset class, but for learning sake we will code these using from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTeBIythFKzR"
   },
   "outputs": [],
   "source": [
    "class PCAMDataset(Dataset):\n",
    "    def __init__(self, data, labels, train):\n",
    "        \"\"\"\n",
    "        Dataset class for the PCAM dataset.\n",
    "        arguments:\n",
    "            data [numpy.array]: all RGB 96-96 images\n",
    "            labels [numpy.array]: corresponding labels\n",
    "            train [bool]: whether the dataset is training or validation\n",
    "        \"\"\"\n",
    "        super(PCAMDataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.train = train\n",
    "\n",
    "        if self.train:\n",
    "            self.augmentation = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((96, 96)),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), #These values are arbitrary\n",
    "                transforms.RandomRotation(20),\n",
    "                transforms.RandomHorizontalFlip()\n",
    "            ])\n",
    "        else:\n",
    "            self.augmentation = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx], self.labels[idx]\n",
    "        return self.augmentation(image), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lfGMUmkKK-M"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M139A5_qJt9N"
   },
   "outputs": [],
   "source": [
    "train_dataset = PCAMDataset(images[train_indices], labels[train_indices], train=True)\n",
    "val_dataset = PCAMDataset(images[val_indices], labels[val_indices], train=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0iDIwc8hWe1"
   },
   "source": [
    "We will now display a random sample of images that have a label of 0 (not containing any tumor tissue) and 1 (containing tumor tissue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_validation_samples = [sample for sample in val_dataset if sample[1] == 1]\n",
    "no_tumor_validation_samples = [sample for sample in val_dataset if sample[1] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_train_samples = [sample for sample in train_dataset if sample[1] == 1]\n",
    "no_tumor_train_samples = [sample for sample in train_dataset if sample[1] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize = (10, 5))\n",
    "random_tumor_sample = tumor_validation_samples[random.randint(0, len(tumor_validation_samples)-1)]\n",
    "random_no_tumor_sample = no_tumor_validation_samples[random.randint(0, len(no_tumor_validation_samples)-1)]\n",
    "\n",
    "axes[0].imshow(random_tumor_sample[0].transpose(2,0), label =\"test\")\n",
    "axes[0].set_title(f\"Random sample containing tumor tissue :{random_tumor_sample[1]}\")\n",
    "axes[1].imshow(random_no_tumor_sample[0].transpose(2,0), label = \"test\")\n",
    "axes[1].set_title(f\"Random sample not containing tumor tissue :{random_no_tumor_sample[1]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like tissues that contains tumors have some colored spot or discrepencies (heterogeneous cells), whereas non tumored tissues seem more homogeneous without speficic details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t65N8O14jjyL"
   },
   "source": [
    "Now we will plot the distribution of class labels in the training and validation datasets, to see how well the classes are balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize =(10,5))\n",
    "\n",
    "axes[0].pie([len(tumor_train_samples), len(no_tumor_train_samples)], labels=[\"Tumor tissue\", \"No Tumor tissue\"], autopct='%.2f%%', shadow=True, colors = [\"gold\", \"peru\"])\n",
    "axes[0].set_title(\"Label distribution in Training Dataset\")\n",
    "axes[1].pie([len(tumor_validation_samples), len(no_tumor_validation_samples)], labels=[\"Tumor tissue\", \"No Tumor tissue\"], autopct='%.2f%%', shadow=True, colors = [\"gold\", \"peru\"])\n",
    "axes[1].set_title(\"Label distribution in Validation Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQIOOG6bKtez"
   },
   "source": [
    "Let's write our first CNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gcw2l7QOLhif"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3,stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,stride=1, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(256*24*24, 512)\n",
    "        self.fc2 = nn.Linear(512,1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6dPrr6lNARI"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device used : {device}')\n",
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-s0dBJ1zMefL"
   },
   "source": [
    "Initialization of the training hyperparameters: We will code the whole training loop, where the model is validated after each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnzU5Q8nMd_9"
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 20\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "metric = tnt.meter.ConfusionMeter(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Znowzr58NmtJ"
   },
   "outputs": [],
   "source": [
    "total_train_losses = []\n",
    "total_val_losses = []\n",
    "total_train_accuracies = []\n",
    "total_val_accuracies = []\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    ##TRAINING##\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    metric.reset()\n",
    "    print(f'Epoch: {epoch}/{num_epochs}')\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        img_batch, lbl_batch = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(img_batch.to(device))\n",
    "        pred = (outputs > 0.5).float().to(device)\n",
    "        loss = criterion(outputs, lbl_batch.float().unsqueeze(1).to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        metric.add(pred.squeeze(1), lbl_batch.long())\n",
    "\n",
    "    total_train_losses.append(np.mean(train_losses))\n",
    "    train_acc=(np.trace(metric.conf)/float(np.ndarray.sum(metric.conf))) *100\n",
    "    total_train_accuracies.append(train_acc)\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    metric.reset()\n",
    "    \n",
    "    for batch in tqdm(val_dataloader):\n",
    "        img_batch, lbl_batch = batch\n",
    "\n",
    "        outputs = model(img_batch.float().to(device))\n",
    "        pred = (outputs > 0.5).float()\n",
    "        loss = criterion(outputs, lbl_batch.float().unsqueeze(1).to(device))\n",
    "        val_losses.append(loss.item())\n",
    "        metric.add(pred.squeeze(1), lbl_batch.long())\n",
    "        \n",
    "    total_val_losses.append(np.mean(val_losses))\n",
    "    val_acc=(np.trace(metric.conf)/float(np.ndarray.sum(metric.conf))) *100\n",
    "    total_val_accuracies.append(val_acc)\n",
    "\n",
    "    print('Confusion Matrix:')\n",
    "    print(metric.conf)\n",
    "    print(f\"Train Loss : {np.mean(train_losses)}, Train Accuracy: {train_acc}\")\n",
    "    print(f\"Validation Loss : {np.mean(val_losses)}, Validation Accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gb5rv7OfN5cx"
   },
   "source": [
    "Now we can validate our model, show that it is not overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(2,1, figsize=(15, 10))\n",
    "axes[0].plot(total_train_accuracies, label = \"Train Accuracy\")\n",
    "axes[0].plot(total_val_accuracies, label = \"Validation Accuracy\")\n",
    "axes[0].set_title(\"Accuracies vs epoch\")\n",
    "axes[0].set_xlabel(\"Epochs\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].legend()\n",
    "axes[1].plot(total_train_losses, label = \"Train Loss\")\n",
    "axes[1].plot(total_val_losses, label = \"Validation Loss\")\n",
    "axes[1].set_title(\"Losses vs epoch\")\n",
    "axes[1].set_xlabel(\"Epochs\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "axes[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the confusion matrix as a metric, to be able to get all the metrics in once (accuracy, FPR, TPR then F1-Score). But here we can only look at the training/validation losses and accuracies and see when the model is not overfitting since we transformed validation data (augmentation) we added some noise which makes the model more efficient on non transformed validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz6CR0NTOTzM"
   },
   "source": [
    "We can try to optimize three hyperparameters (the learning rate, the batch size and the number of layers in your CNN model), to see it improves the efficiency of the model.\n",
    "\n",
    "To do so, we use bayesian optimization to find the best set of hyperparameters using `scikit-optimize` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vS0Le5J9lrOB"
   },
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Integer, Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfL3_nBohCn4"
   },
   "outputs": [],
   "source": [
    "dimensions = [\n",
    "    Real(1e-5, 1e-1, prior=\"log-uniform\", name=\"learning_rate\"),\n",
    "    Categorical([16, 32, 64, 128, 256], name=\"batch_size\")\n",
    "]\n",
    "parameters_default_values = [\n",
    "    0.001, #lr\n",
    "    32 #BATCHSIZE\n",
    "]\n",
    " # default value for each parameter for initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TH0J6f0ul7Q8"
   },
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=dimensions)\n",
    "def fit_opt(learning_rate, batch_size):\n",
    "    model = ConvNet().to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    \n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        img_batch, lbl_batch = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(img_batch.to(device))\n",
    "        loss = criterion(outputs, lbl_batch.float().unsqueeze(1).to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            img_batch, lbl_batch = batch\n",
    "            outputs = model(img_batch.to(device))\n",
    "            val_loss += criterion(outputs, lbl_batch.float().unsqueeze(1).to(device)).item()\n",
    "\n",
    "    score = val_loss / len(val_loader) #We will minimize the mean validation loss\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BJv8j5ml_qK"
   },
   "outputs": [],
   "source": [
    "gp_result = gp_minimize(\n",
    "    func=fit_opt,            # Function to minimize\n",
    "    dimensions=dimensions,   # Search space\n",
    "    x0=parameters_default_values,\n",
    "    n_calls=11,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    "    )\n",
    "\n",
    "print(f\"Optimal set of parameters found at iteration {np.argmin(gp_result.func_vals)}\")\n",
    "print(gp_result.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that regarding the parameters we use we get a better 1-epoch loss even if this method is a bit empiric and depends on the batch etc. Also if we had more computing ressource we could do many more grid search cross validation to fine tune the model, but here we are limited. \n",
    "\n",
    "We couldn't fine tune the number of layers used in the convolutional layer, but it can be a very impactful hyperparameter to finetune (same for the probabilty of dropout, number of maxpool, size of the Fully connected layers)\n",
    "\n",
    "However let's plot the losses vs batch sizes and learning rates (with the other parameter fixed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = np.array(gp_result.x_iters)[:,0]\n",
    "sorted_args_lr = np.argsort(learning_rates)\n",
    "batch_sizes = np.array(gp_result.x_iters)[:,1]\n",
    "sorted_args_bs = np.argsort(batch_sizes)\n",
    "\n",
    "_, axes = plt.subplots(1,2, figsize = (10,5))\n",
    "axes[0].plot(np.sort(learning_rates), gp_result.func_vals[sorted_args_lr], label=\"Learning Rates Losses\")\n",
    "axes[0].set_xlabel(\"Learning rate\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Loss vs learning rates\")\n",
    "axes[0].legend()\n",
    "axes[1].plot(np.sort(batch_sizes), gp_result.func_vals[sorted_args_bs], label=\"Batch sizes Losses\", color = 'red')\n",
    "axes[1].set_xlabel(\"Batch size\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "axes[1].set_title(\"Loss vs learning rates\")\n",
    "axes[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the batch sizes, it's hard to discuss on the efficiency of the chosen parameters, because it's for fixed learning rate. Here the learning rate is way more impactful on the losses which makes sens because a big learning rate causes the loss function to not converge at all.\n",
    "\n",
    "Also instead of using self made convnet we can use well known architectures such as vgg16 which (after some research) is very efficient on this kind of classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwOO_DytqktF"
   },
   "source": [
    "### Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saliency Map & GradCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoI3IQwMk1A-"
   },
   "source": [
    "With the exception of using Saliency maps, we can use one other interpretability method such as GradCAM\n",
    "With respect to the code block below, saliency maps are useful in interpreting the decisions of CNNs. However, they have some limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzjgsTYmGJy2"
   },
   "outputs": [],
   "source": [
    "## Code block to use saliency maps\n",
    "import cv2\n",
    "\n",
    "image = images[40]\n",
    "label = labels[40]\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "              ]) ### Here put the transforms to be applied\n",
    "\n",
    "input_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Set the requires_grad attribute of the input tensor to True for gradients\n",
    "input_tensor.requires_grad_(True)\n",
    "\n",
    "# Forward pass to get the model prediction\n",
    "output = model(input_tensor.to(device))\n",
    "\n",
    "# Choose the class index for which you want to visualize the saliency map\n",
    "class_index = torch.argmax(output)\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "# Backward pass to get the gradients of the output w.r.t the input\n",
    "output[0, class_index].backward()\n",
    "\n",
    "# Get the gradients from the input tensor\n",
    "saliency_map = input_tensor.grad.squeeze(0).abs().cpu().numpy()\n",
    "\n",
    "# Normalize the saliency map for visualization (optional)\n",
    "saliency_map = saliency_map / saliency_map.max()\n",
    "\n",
    "normalized_saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
    "\n",
    "# Convert the saliency map back to a uint8 image format (0-255)\n",
    "saliency_map_image = np.uint8(255 * normalized_saliency_map)\n",
    "\n",
    "# Aggregate across the channels\n",
    "aggregate_saliency = saliency_map.sum(axis=0)\n",
    "\n",
    "# Plot the input image and its corresponding saliency map side by side\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot the input image\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title(f'Input Image, with {label} tumor')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Plot the saliency map\n",
    "axes[1].imshow(aggregate_saliency, cmap='jet', alpha=0.7)  # Overlay saliency map on the input image\n",
    "axes[1].imshow(image, alpha=0.3)  # Overlay input image for comparison\n",
    "axes[1].set_title('Saliency Map')\n",
    "axes[1].axis('off')\n",
    "\n",
    "## Grad-CAM\n",
    "#The following function registers the gradient and activations of our last layer\n",
    "def register_hooks(layer):\n",
    "    gradients = []\n",
    "    activations = []\n",
    "\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        gradients.append(grad_output[0])\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        activations.append(output)\n",
    "\n",
    "    layer.register_forward_hook(forward_hook)\n",
    "    layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    return gradients, activations\n",
    "\n",
    "#We compute them here\n",
    "gradients, activations = register_hooks(model.conv4)\n",
    "\n",
    "# Forward pass to get the model prediction\n",
    "output = model(input_tensor.to(device))\n",
    "\n",
    "# Choose the class index for which you want to visualize the Grad-CAM (this time)\n",
    "target_class = torch.argmax(output)\n",
    "\n",
    "# Backward pass to get the gradients of the output w.r.t the input\n",
    "model.zero_grad()\n",
    "output[0, target_class].backward()\n",
    "\n",
    "# Extract the gradients and activations\n",
    "gradients = gradients[0].detach() \n",
    "activations = activations[0].detach() \n",
    "\n",
    "# Compute the heatmap\n",
    "weights = gradients.mean(dim=[2, 3], keepdim=True)\n",
    "gradcam = F.relu((weights * activations).sum(dim=1)).squeeze(0) \n",
    "gradcam -= gradcam.min()\n",
    "gradcam /= gradcam.max()\n",
    "\n",
    "gradcam_resized = cv2.resize(gradcam.cpu().numpy(), (image.shape[0], image.shape[1]))\n",
    "\n",
    "# Plot the Grad-CAM heatmap\n",
    "axes[2].imshow(image)\n",
    "axes[2].imshow(gradcam_resized, cmap='jet', alpha=0.5)\n",
    "axes[2].set_title(\"Grad-CAM Heatmap\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grad-CAM generates a heatmap by computing the gradients of the target class with respect to the feature maps of a specific convolutional layer label, it uses these gradients to weight the activations, highlighting regions of the image that strongly influence the model's classification.\n",
    "\n",
    "Here, it seems that the Grad-CAM heatmap highlights biologically relevant regions, such as small tissue structures, which align with human intuition for the task. The results appear to make sense for this classification since it's focusing on hetergeneous regions. It's hard to really interpret without the knowledge of a specialist, so it's only an intuition.\n",
    "\n",
    "The limitations, we can see on this example that interpretations can be noisy or maybe focus irrelevant things (especially in the Saliency Map below)\n",
    "\n",
    "Also Grad-CAM depends on the choice of the layer which may not align with meaningful features and CNN remains black boxes at higher levels so complete transparency can be challenging. Some versions of Grad-CAM improve it such as Grad CAM ++ which refine the heatmap by considering the importance of individual pixels for multilple instance of the same class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "\n",
    "# Preprocess the input image\n",
    "image = images[40]\n",
    "label = labels[40]\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "input_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Convert the input tensor to a numpy array for SHAP\n",
    "input_numpy = input_tensor.squeeze(0).permute(1, 2, 0).numpy()\n",
    "\n",
    "# Define a SHAP explainer for the model\n",
    "explainer = shap.DeepExplainer(model, torch.zeros_like(input_tensor).to(device))\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = explainer.shap_values(input_tensor.to(device))\n",
    "\n",
    "# Aggregate SHAP values across channels\n",
    "shap_values = np.array(shap_values[0])  # Extract SHAP values for the first output class\n",
    "aggregate_shap = shap_values.sum(axis=1).squeeze(0)\n",
    "\n",
    "# Normalize the SHAP values for visualization\n",
    "aggregate_shap_normalized = (aggregate_shap - aggregate_shap.min()) / (aggregate_shap.max() - aggregate_shap.min())\n",
    "\n",
    "# Convert the SHAP values to a uint8 image format (0-255)\n",
    "shap_map_image = np.uint8(255 * aggregate_shap_normalized)\n",
    "\n",
    "# Resize SHAP values to match the original image size\n",
    "shap_map_resized = cv2.resize(aggregate_shap_normalized, (image.shape[1], image.shape[0]))\n",
    "\n",
    "# Plot the input image and its corresponding SHAP map side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the input image\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title(f'Input Image, with {label} tumor')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Plot the SHAP map\n",
    "axes[1].imshow(image, alpha=0.3)  # Overlay input image for comparison\n",
    "axes[1].imshow(shap_map_resized, cmap='jet', alpha=0.7)  # SHAP heatmap\n",
    "axes[1].set_title('SHAP Heatmap')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP (SHapley Additive exPlanations) is a popular method for interpreting machine learning models by attributing the contribution of each feature to a model's prediction. Based on game theory's Shapley values, SHAP calculates the marginal contribution of each feature by comparing the prediction when the feature is included versus excluded. It ensures fair distribution of contributions by considering all possible feature combinations. This method works with a variety of model types and can provide global insights into feature importance or local explanations for individual predictions. In computer vision, SHAP is adapted to handle image data, highlighting regions of an image that most influence the model's output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning a Visual Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ViT, self).__init__()\n",
    "        self.vit = models.vit_b_16(pretrained=True)\n",
    "        self.vit.heads = nn.Linear(self.vit.heads.head.in_features, 1) #The head is custom here\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.vit(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device used : {device}')\n",
    "model = ViT().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can add a metric as before\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224))\n",
    "])\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(transform(inputs))\n",
    "        loss = criterion(outputs, labels.float().unsqueeze(0).T)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_dataloader)}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            batch = inputs, labels\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss / len(val_dataloader)}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "torch.save(model.state_dict(), 'tumor_vit_model.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
